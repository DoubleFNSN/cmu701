\section{Decision Boundary of Naive Bayes (10 Points) (Yuntian)}
Consider the problem of predicting a label $Y \in \mathcal{Y}$ given an input feature vector $X \in \mathcal{X}$. Suppose $\mathcal{X} = \mathbb{R}^d$, $Y \in \{0, 1\}$. We make the Naive Bayes assumption that features are conditionally independent given label, i.e. $P(X | Y) = \prod_{i=1}^d P(X_i |Y)$, where $X_i$ denotes the $i$-th component of $X$. In addition, we impose a Bernoulli prior on $Y$: $P(Y=1) = \pi$. 

(a) Assume that $P(X_i | Y)$ is in the exponential family: $P(X_i = x_i | Y = y) = h_i(x_i) \exp (\theta_{iy} \cdot T_i(x_i) - A_{i}(\theta_{iy}))$. Compute the posterior distribution $P(Y | X)$. Compute the decision boundary $\{x \in \mathcal{X}: P(Y=1 | X = x) = P(Y=0 | X =x)\}$. (You can use sigmoid function $\sigma(x)=\frac{1}{1+e^{-x}}$ to simplify the expression).

(b) Assume that $P(X_i | Y=y)$ is a Gaussian distribution with mean $\mu_{iy}$ and variance $\sigma_{i}^2$ (Note that the variance here does not depend on label $Y$), show that the decision boundary is linear in terms of $X$. (Hint: Recall from Problem 1.1 that Gaussian distribution is in the exponential family).


\section{KNN Classification (15 Points) (Yuntian)}
Consider a classification problem using kNN. We have $N$ training points $x_1, x_2, \cdots x_N$ and corresponding labels $y_1, y_2, \cdots y_N$. If we wish to classify a new data point, the classification rule is simply a majority vote among its $k$ nearest neighbours in the training set.

(a) Consider $k=1$ case, is it possible to build a decision tree (the decision at each node can only take the form of {\color{red}"$x^i \le t$ or $x^i > t$" where $x^i$ denotes the i-th feature of $x$, $t$ is a real number and can be different in different nodes}) which behaves exactly the same as the 1-NN classifier?

(b) Recall from class that the decision rule of kNN can be viewed using a probabilistic interpretation: Suppose we have a data set comprising of $N_k$ points in class $C_k$. For a new point $x$, we draw a ball around $x$ containing exactly $K$ nearest neighbours. Suppose this ball has volume $V$ and contains $K_k$ points from class $C_k$. Then we can estimate the density conditioned on class $C_k$ as:

$$
P(x | C_k) = \frac{K_k}{N_k V}
$$

Similarly, we estimate the unconditioned density as:

$$
P(x) = \frac{K}{N V}
$$

We also estimate the prior as:

$$
P(C_k) = \frac{N_k}{N}
$$

In class we have shown that under this model, Bayes decision rule will yield kNN classification. Show that the density $P(x)$ given by this model is an improper distribution whose integral over all space is not guaranteed to sum up to 1. (You only need to construct a special case to show that).

(c) In kNN classification, the nearest neighbours of a given test point depend on the distance metric. We often implicitly use Euclidean distance as the distance metric, but we can use other distance metrics as well. Consider a binary classification problem using 1-NN. Assume the labels can only be $C_1$ and $C_2$. Denote the number of training points as $N$. For a test point $x$, denote its nearest neighbour as $x'$ (note that the nearest neighbour depends on the distance metric), then the probability that $x$ is misclassified is:

$$
P_N(e|x) = P(C_1 | x) P(C_2 | x') + P(C_2 | x) P(C_1 | x')
$$

The asymptotic error rate (i.e. the error rate when the size of training set is infinite) is:

$$
P(e|x) = \lim_{N \to \infty} P_N (e|x)
$$

It can be shown that when $N\to \infty$, the asymptotic error rate $P(e|x)$ of 1-NN is upper bounded by twice the minimum achievable error rate. However, what we care about is $P_N(e|x)$. Therefore, if we can bound the difference between $P_N(e|x)$ and $P(e|x)$, then $P_N(e|x)$ is also upper bounded (As $P_N(e|x)$ depends on the training set, we can only bound it in a probabilistic sense, but that's not the focus of this problem). Our objective here is to use a proper distance metric to minimize the expected squared difference between $P(e|x)$ and $P_N(e|x)$: $\min \mathbb{E} [(P_N(e|x) - P(e|x))^2 | x]$. Note that the expectation here is taken with respect to the training set, conditioned on the test point $x$.

Denote $\nabla P(C_1 |x) \equiv \frac{\partial P(C_1|x)}{\partial x}$. Approximate $P(C_1 |x')$ by the first order Taylor expansion at $x$: $P(C_1 |x') \simeq  P(C_1 | x) + \nabla P(C_1 |x)^T (x'-x)$, show that $\mathbb{E} [(P_N(e|x) - P(e|x))^2 | x]$ is minimized by using the distance metric $d(x, x') \equiv |\nabla P(C_1 |x)^T (x - x')|$.

Note: In practice, we can estimate the direction of $\nabla P(C_1 |x)$ from training data. Denote it as $\hat{\nabla}$, then we can use $d(x, x') \equiv |\hat{\nabla}^T (x - x')|$ as the distance metric, because the scaling of $\nabla P(C_1 |x)$ does not affect the nearest neighbour.

\section{Decision Trees (25 Points) (Yuntian)}
\subsection{Decision tree on two features}
Consider training a decision tree on $n$ two dimensional vectors $x = (x_1,x_2)$.

(a) Assume we have two equal vectors $x$ and $x'$ in our training set (that is, all attributes of $x$ and $x'$ including the labels are exactly the same). Can removing $x'$ from our training data change the decision tree we learn for this dataset? Explain briefly.

(b) Assume that the training instances are linearly separable. That is, there exists a $\{w, b\}$ such that

$$
y=\left\{
\begin{array}{rcl}
+1       &      & {w^T x + b > 0}\\
-1     &      & {w^T x + b \le 0}
\end{array} \right.
$$

Can a decision tree correctly classify these vectors? ({\color{red}the decision at each node can only take the form of "$x_i \le t$ or $x_i > t$" where $x_i$ is a feature, $t$ is a real number and can be different in different nodes}). If so, what is an upper bound on the depth of the corresponding decision tree (as tight as possible)? If not, why not?

(c) Now assume that these n inputs are not linearly separable (that is, no $\{w, b\}$ exists for correctly classifying all inputs using the above rule). Can a decision tree correctly classify these vectors? ({\color{red}the decision at each node can only take the form of "$x_i \le t$ or $x_i > t$" where $x_i$ is a feature, $t$ is a real number and can be different in different nodes}) If so, what is an upper bound on the depth of the corresponding decision tree (as tight as possible)? If not, why not?

\subsection{C4.5 Algorithm}
\begin{table}[h]
    \centering
        \begin{tabular}{|c|cccc|c|}
            \hline
            Day & Outlook & Temperature & Humidity & Wind & Play \\
            \hline
            D1 & Sun & 26 & High & Low & No \\
            D2 & Sun & 25 & High & High & No \\ 
            D3 & Overcast & 25 & High & Low & Yes \\
            D4 & Rain & 24 & High & Low & Yes \\
            D5 & Rain & 19 & Normal & Low & Yes \\ 
            D6 & Rain & 20 & Normal & High & No \\
            D7 & Overcast & 20 & Normal & High & Yes \\
            D8 & Sun & 23 & High & Low & No \\
            D9 & Sun & 20 & Normal & Low & Yes \\
            D10 & Rain & 25 & Normal & Low & Yes \\
            D11 & Sun & 24 & Normal & High & Yes \\
            D12 & Overcast & 22 & High & High & Yes \\
            D13 & Overcast & 23 & Normal & Low & Yes \\
            D14 & Rain & 23 & High & High & No \\
            \hline
        \end{tabular}
\caption{Dataset}
\label{tab: C45}
\end{table}

We have learnt ID3 algorithm in class. One limitation of ID3 is that it is overly sensitive to features with large numbers of values. For example, if each training instance has a unique id, then the information gain will be maximized by using this id as feature, while this is not useful. C4.5 algorithm improves this by using information gain ratio instead of information gain to evaluate features. Denote the feature for an input by $X$, and its label by $Y$. Recall that in ID3, we choose feature $X$ that maximizes information gain $IG(X)$:

$$
IG(X) \equiv  H(Y) - H(Y|X)
$$

Now we define split information as follows: Suppose there are$|D|$ samples at the current node, and after splitting by feature $X$, they fall into $V$ child nodes. Assume the number of samples that pass through each child node is $|D_1|, |D_2|, \cdots |D_V|$, then the split information of feature $X$ is

$$
SplitInfo(X) \equiv - \sum_{j=1}^V \frac{|D_j|}{|D|} \log \frac{|D_j|}{|D|}
$$

The information gain ratio is defined as 

$$
GainRatio(X) \equiv \frac{IG(X)}{SplitInfo(X)}
$$

C4.5 algorithm uses information gain ratio to determine the best splitting attribute. The intuition is that $SplitInfo(X)$ acts as a normalizer to penalize features with a large number of values. For example, if $\forall i,j$ we have $|D_i| = |D_j|$, then $SplitInfo(X)=\log V$, so features with smaller $V$ is preferred. 

Draw the decision tree of the dataset in table \ref{tab: C45}\footnote{\url{http://saiconference.com/Downloads/SpecialIssueNo10/Paper_3-A_comparative_study_of_decision_tree_ID3_and_C4.5.pdf}} using both the ID3 and C4.5 algorithms. The features here are Outlook, Temperature, Humidity and Wind, and the label to be predicted is Play. Treat Temperature as a discrete feature.