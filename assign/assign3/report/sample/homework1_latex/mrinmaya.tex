\section{Probability and Statistics Review (25 Points) (Mrinmaya)}
\subsection{Random Variable, Probability Density Function and Expectation}
First, we will review three important concepts of probability that will be repeatedly used in this class: \textit{Random Variables}, \textit{Probability Mass/Density Functions (pmf/pdf)} and \textit{Expectations} in the context of the normal distribution.

\textit{Random Variables} is a variable whose value is subject to variations due to chance or randomness. You can think of a random variable $X\colon \Omega \to E$ as a measurable function from the set of possible outcomes $\Omega$ to some set $E$. For all practical purposes, $X$ is a real-valued function (i.e. $E=\mathbb{R}$). The probabilities of different outcomes or sets of outcomes (events) are given by the probability measure $P$ with which $\Omega$ is equipped. $X$ describes some numerical property that outcomes in $\Omega$ may have. E.g. the number of heads in a random collection of coin flips; the height of a random person, etc.

\textit{Probability Mass/Density Function (pmf/pdf)} of a random variable $X$ (usually denoted as $f_X(x)$) is a function that describes the relative likelihood for this random variable $X$ to take on a given value $x$ i.e. $f_X(x) = P(X=x)$. If the random variable $X$ is discrete, the function is called \textit{Probability Mass Function}, else if the random variable $X$ is continuous, the function is called \textit{Probability Density Function}.

\textit{Expectation} or expected value of a random variable $X$ (usually denoted as $\mathbb{E}[X]$) is intuitively the long-run average value of repetitions of the experiment it represents. The experiment being drawing a value of the random variable under the probability mass/density function associated with it. Mathematically, $\mathbb{E}[X] = \int_{-\infty}^\infty x f_X(x)\, \mathrm{d}x$ for the continuous case (in discrete case, you simply replace the integral with summation). As an example, you can verify that the expected value of a dice roll is 3.5.

One of the most commonly used distribution in Statistics and Machine Learning is the Normal Distribution. In the multivariate setting (where $\mathbf{X}$ and $\mathbf{x}$ are both vectors), we say that $\mathbf{X} \sim \mathcal{N}(\mathbf{\mu}, \mathbf{\Sigma})$ iff
$f_{\mathbf X}(\mathbf{x}) = \frac{1}{\sqrt{(2\pi)^{k}|\boldsymbol\Sigma|}} \exp\left(-\frac{1}{2}({\mathbf x}-{\boldsymbol\mu})^\mathrm{T}{\boldsymbol\Sigma}^{-1}({\mathbf x}-{\boldsymbol\mu}) \right)$. Here, $k$ is the dimensionality of $\mathbf{X}$.

\begin{enumerate}
\item If $\mathbf{Z} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$ where $\mathbf{I}$ is the identity matrix, show that $\mathbf{X} = \mathbf{\mu}+ \mathbf{\Sigma}^{1/2}\mathbf{Z} \sim \mathcal{N}(\mathbf{\mu}, \mathbf{\Sigma})$
\\Note: For sake of simplicity, you can assume that the dimensionality of $\mathbf{X}$ is 2, but this result holds in general. Note that the reverse holds too.
\item Compute the first three moments of $\mathbf{X}$. In other words, compute $\mathbb{E}(X^p)$ for $p = 1, 2, 3$.
\item If $\mathbf{X} \sim \mathcal{N}(\mathbf{\mu}, \mathbf{\Sigma})$ and suppose we partition the random vector $\mathbf{X}$ as $\mathbf{X} = \left(\mathbf{X_a}, \mathbf{X_b}\right)^T$. Similarly, let $\mathbf{\mu} = \left(\mathbf{\mu_a}, \mathbf{\mu_b}\right)^T$ and $\mathbf{\Sigma} = \begin{bmatrix}
    \mathbf{\Sigma_{aa}} & \mathbf{\Sigma_{ab}} \\
    \mathbf{\Sigma_{ba}} & \mathbf{\Sigma_{bb}}
\end{bmatrix}$.
\\Note: For sake of simplicity, you can assume that the dimensionality of $\mathbf{X}$ is 2 and the dimensionality of $\mathbf{X_a}$ and $\mathbf{X_b}$ is 1, but again, these results hold in general.
\begin{enumerate}
\item Show that the marginal distribution of $\mathbf{X_a}$ is $\mathbf{X_a} \sim \mathcal{N}(\mathbf{\mu_a}, \mathbf{\Sigma_{aa}})$.
\item Show that the conditional distribution of $\mathbf{X_b}$ given $\mathbf{X_a} = \mathbf{x_a}$ is:\\ $\mathbf{X_b}|\mathbf{X_a} = \mathbf{x_a} \sim \mathcal{N}(\mu_b+\Sigma_{ba}\Sigma_{aa}^{-1}(x_a-\mu_a), \Sigma_{bb}-\Sigma_{ba}\Sigma_{aa}^{-1}\Sigma_{ab})$
\end{enumerate}
\end{enumerate}

\subsection{Prior, Likelihood and Posterior}

\subsection{Maximum Likelihood Estimation}